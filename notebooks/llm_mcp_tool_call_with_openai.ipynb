{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LLM Tool Calling + (Simulated) MCP with **Real OpenAI Model** \ud83e\udde0\ud83d\udee0\ufe0f\n",
        "\n",
        "This notebook is a **companion** to the step\u2011by\u2011step simulation demo.\n",
        "\n",
        "Here we plug in a **real OpenAI model** to show how tool calling actually works in code,\n",
        "while still keeping the MCP side **simulated and simple**.\n",
        "\n",
        "You will see:\n",
        "\n",
        "1. A simulated **MCP server** that defines tools (`fake_weather`, `echo`).\n",
        "2. An **MCP client** that:\n",
        "   - discovers tools (via a Python list, as if from `list_tools()`),\n",
        "   - converts them into the OpenAI `tools` schema.\n",
        "3. A real **OpenAI chat completion call** with `tools=...`.\n",
        "4. The model returning **tool calls** (tool name + JSON args, no schema).\n",
        "5. The client executing the tools locally (simulating MCP `call_tool`).\n",
        "6. A second OpenAI call to turn tool results into a final answer.\n",
        "\n",
        "> \u26a0\ufe0f You need an `OPENAI_API_KEY` set in your environment for this notebook to work.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Setup \u2013 Install and Configure OpenAI Client\n",
        "\n",
        "Run the following cell **once** to install the OpenAI Python client (if not already installed).\n",
        "Then make sure your `OPENAI_API_KEY` environment variable is set.\n",
        "\n",
        "In a terminal this typically looks like:\n",
        "\n",
        "```bash\n",
        "export OPENAI_API_KEY=\"sk-...\"\n",
        "```\n",
        "\n",
        "In Colab, you can set it in `os.environ` as well.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# If needed, install the OpenAI client library\n",
        "# !pip install --upgrade openai\n",
        "\n",
        "import os, json\n",
        "from typing import Callable\n",
        "from openai import OpenAI\n",
        "\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    print(\"\u26a0\ufe0f WARNING: OPENAI_API_KEY is not set. Set it before calling the API.\")\n",
        "else:\n",
        "    print(\"\u2705 OPENAI_API_KEY is set.\")\n",
        "\n",
        "# Choose a tool\u2011calling capable model.\n",
        "MODEL = \"gpt-4o-mini\"  # change if you prefer a different model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Simulated MCP Server: Tool Definitions\n",
        "\n",
        "We simulate what an MCP server would expose via `list_tools()`.\n",
        "\n",
        "Each tool has:\n",
        "- a **name** (e.g., `fake_weather`),\n",
        "- a **description**, and\n",
        "- an `inputSchema` (JSON schema for parameters).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "mcp_tools = [\n",
        "    {\n",
        "        \"name\": \"fake_weather\",\n",
        "        \"description\": \"Return a rough current temperature for a city.\",\n",
        "        \"inputSchema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"city\": {\"type\": \"string\", \"description\": \"City name\"}\n",
        "            },\n",
        "            \"required\": [\"city\"],\n",
        "        },\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"echo\",\n",
        "        \"description\": \"Echo back a message.\",\n",
        "        \"inputSchema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"message\": {\"type\": \"string\", \"description\": \"Message to echo\"}\n",
        "            },\n",
        "            \"required\": [\"message\"],\n",
        "        },\n",
        "    },\n",
        "]\n",
        "\n",
        "print(\"Simulated MCP tools (like list_tools() output):\")\n",
        "for t in mcp_tools:\n",
        "    print(f\"- {t['name']}: {t['description']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. MCP Client: Convert MCP Tools \u2192 OpenAI Tool Schema\n",
        "\n",
        "The MCP client must convert the MCP tool descriptions into the format expected by\n",
        "OpenAI's tool\u2011calling interface.\n",
        "\n",
        "That is exactly what `mcp_tools_to_openai_tools(...)` does.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def mcp_tools_to_openai_tools(mcp_tools: list[dict]) -> list[dict]:\n",
        "    \"\"\"Convert simulated MCP tools into OpenAI\u2011style tools list.\"\"\"\n",
        "    openai_tools = []\n",
        "    for tool in mcp_tools:\n",
        "        openai_tools.append(\n",
        "            {\n",
        "                \"type\": \"function\",\n",
        "                \"function\": {\n",
        "                    \"name\": tool[\"name\"],\n",
        "                    \"description\": tool[\"description\"],\n",
        "                    \"parameters\": tool[\"inputSchema\"],\n",
        "                },\n",
        "            }\n",
        "        )\n",
        "    return openai_tools\n",
        "\n",
        "openai_tools = mcp_tools_to_openai_tools(mcp_tools)\n",
        "\n",
        "print(\"OpenAI tools schema that will be sent to the model:\\n\")\n",
        "print(json.dumps(openai_tools, indent=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. First OpenAI Call: Model Decides Whether to Use Tools\n",
        "\n",
        "Now we perform the **first real OpenAI chat completion** call.\n",
        "\n",
        "We give the model:\n",
        "- the user query\n",
        "- the list of tools\n",
        "\n",
        "The model may respond with `tool_calls` containing:\n",
        "- tool name\n",
        "- JSON arguments\n",
        "\n",
        "But it will **not** send back any schema.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "client = OpenAI()\n",
        "\n",
        "user_query = \"What is the weather in London right now?\"\n",
        "messages = [{\"role\": \"user\", \"content\": user_query}]\n",
        "\n",
        "print(\"Sending first request to OpenAI with tools...\\n\")\n",
        "first_response = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=messages,\n",
        "    tools=openai_tools,\n",
        ")\n",
        "\n",
        "first_message = first_response.choices[0].message\n",
        "print(\"Raw first message from model (repr):\\n\")\n",
        "print(first_message)\n",
        "\n",
        "tool_calls = first_message.tool_calls or []\n",
        "if tool_calls:\n",
        "    print(\"\\n\u2705 Model requested tool calls:\")\n",
        "    for tc in tool_calls:\n",
        "        print(f\"- Tool name: {tc.function.name}\")\n",
        "        print(f\"  Arguments JSON: {tc.function.arguments}\")\n",
        "else:\n",
        "    print(\"\\n\u2139\ufe0f Model did not request any tool calls.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Define Tool Implementations (Simulating MCP `call_tool`)\n",
        "\n",
        "Now we define the actual Python functions that act like MCP server tools.\n",
        "The client will use these when the model asks for a tool.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def tool_fake_weather(city: str) -> str:\n",
        "    temps = {\"London\": 18, \"Hyderabad\": 32, \"Bangalore\": 26}\n",
        "    temp = temps.get(city, 25)\n",
        "    return f\"It is roughly {temp}\u00b0C in {city} right now.\"\n",
        "\n",
        "\n",
        "def tool_echo(message: str) -> str:\n",
        "    return f\"Echo: {message}\"\n",
        "\n",
        "\n",
        "TOOL_REGISTRY: dict[str, Callable[..., str]] = {\n",
        "    \"fake_weather\": tool_fake_weather,\n",
        "    \"echo\": tool_echo,\n",
        "}\n",
        "\n",
        "\n",
        "def execute_tool_call_from_model(tool_call) -> dict:\n",
        "    \"\"\"Execute a tool call returned by the model and build a tool message.\n",
        "\n",
        "    In a real MCP setup, this is where MCP protocol would be used.\n",
        "    Here we just call local Python functions.\n",
        "    \"\"\"\n",
        "    tool_name = tool_call.function.name\n",
        "    args = json.loads(tool_call.function.arguments or \"{}\")\n",
        "    fn = TOOL_REGISTRY.get(tool_name)\n",
        "    if fn is None:\n",
        "        content = f\"Error: unknown tool '{tool_name}'\"\n",
        "    else:\n",
        "        try:\n",
        "            content = fn(**args)\n",
        "        except Exception as e:  # noqa: BLE001\n",
        "            content = f\"Error executing tool {tool_name}: {e}\"\n",
        "\n",
        "    tool_message = {\n",
        "        \"role\": \"tool\",\n",
        "        \"tool_call_id\": tool_call.id,\n",
        "        \"content\": content,\n",
        "    }\n",
        "    return tool_message\n",
        "\n",
        "\n",
        "tool_messages: list[dict] = []\n",
        "for tc in tool_calls:\n",
        "    tm = execute_tool_call_from_model(tc)\n",
        "    tool_messages.append(tm)\n",
        "\n",
        "print(\"Tool messages generated after executing tools:\\n\")\n",
        "print(json.dumps(tool_messages, indent=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Second OpenAI Call: Final Answer Using Tool Results\n",
        "\n",
        "Now we build up the **full conversation history**:\n",
        "\n",
        "1. User message.\n",
        "2. Assistant's tool\u2011call message (from the first response).\n",
        "3. One or more `role=\"tool\"` messages with the tool outputs.\n",
        "\n",
        "We send this to OpenAI and ask it to write the final answer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "full_messages: list[dict] = []\n",
        "full_messages.append({\"role\": \"user\", \"content\": user_query})\n",
        "\n",
        "if tool_calls:\n",
        "    # Add assistant message that contained the tool calls\n",
        "    full_messages.append({\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": first_message.content or \"\",\n",
        "        \"tool_calls\": [tc.to_dict() for tc in tool_calls],\n",
        "    })\n",
        "    # Add tool result messages\n",
        "    full_messages.extend(tool_messages)\n",
        "else:\n",
        "    # If no tool calls, just include the assistant content\n",
        "    full_messages.append({\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": first_message.content or \"\",\n",
        "    })\n",
        "\n",
        "print(\"Sending second request to OpenAI with tool results...\\n\")\n",
        "second_response = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=full_messages,\n",
        ")\n",
        "\n",
        "final_message = second_response.choices[0].message\n",
        "print(\"Final assistant message:\\n\")\n",
        "print(final_message)\n",
        "\n",
        "print(\"\\nAs plain text:\\n\")\n",
        "print(final_message.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Summary: What Students Should Take Away\n",
        "\n",
        "1. **MCP server** defines tools and their JSON schemas.\n",
        "2. **MCP client** converts those tools into OpenAI's `tools` format.\n",
        "3. **First OpenAI call**:\n",
        "   - Model sees the tools and user query.\n",
        "   - It may return `tool_calls` with **tool name + JSON arguments**.\n",
        "   - It does **not** send back the schema.\n",
        "4. **Client executes tools**:\n",
        "   - In reality, via MCP protocol; here, via local Python functions.\n",
        "   - Builds `role=\"tool\"` messages with the outputs.\n",
        "5. **Second OpenAI call**:\n",
        "   - Model sees the conversation + tool messages.\n",
        "   - Produces a natural language answer.\n",
        "\n",
        "\ud83d\udc49 The key illusion:\n",
        "\n",
        "> The LLM does not \"speak MCP\". The **client** does.\n",
        ">\n",
        "> The LLM just\n",
        ">  - reads tool schemas,\n",
        ">  - chooses a tool,\n",
        ">  - fills JSON arguments,\n",
        ">  - and uses the tool results to answer.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}