{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LLM Tool Calling + MCP Concept Demo \ud83e\udde0\ud83d\udee0\ufe0f\n",
        "\n",
        "This notebook **step by step** explains what really happens when an LLM:\n",
        "\n",
        "1. Receives a user query.\n",
        "2. Sees a list of tools (e.g., coming from an MCP server via an MCP client).\n",
        "3. Decides to call a tool.\n",
        "4. Returns only **tool name + JSON arguments** (not schemas).\n",
        "5. Lets the **client** execute the tool and give results back.\n",
        "6. Produces a final natural-language answer.\n",
        "\n",
        "We will **simulate** the OpenAI + MCP behavior so that you can run this without any API keys.\n",
        "\n",
        "The focus is on **clarity and intuition**, not on networking or real HTTP calls.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Big Picture: Who Does What?\n",
        "\n",
        "We care about the path from:\n",
        "\n",
        "> **User query \u2192 tool selection \u2192 tool execution \u2192 final answer**\n",
        "\n",
        "In a real system with MCP, you have:\n",
        "\n",
        "- **MCP Server** \u2013 defines tools (capabilities), e.g. `fake_weather(city)`.\n",
        "- **MCP Client** \u2013 connects to the server, calls `list_tools()` & `call_tool(...)`.\n",
        "- **LLM (OpenAI)** \u2013 sees tools as function schemas and decides *which* tool to call.\n",
        "\n",
        "In this notebook, we simulate this flow:\n",
        "\n",
        "1. A *fake MCP server* that just holds tool definitions in Python.\n",
        "2. An *MCP client* that:\n",
        "   - discovers tools,\n",
        "   - converts them to **OpenAI-style tool schemas**,\n",
        "   - executes the tools when the model asks.\n",
        "3. A *fake LLM* that behaves like a tool-calling model:\n",
        "   - first call: chooses a tool and JSON arguments,\n",
        "   - second call: uses tool results to generate the final answer.\n",
        "\n",
        "We will trace everything with prints so students can see **exactly** what is happening.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Simulated MCP Server: Defining Tools\n",
        "\n",
        "In a real MCP server, you would write Python like:\n",
        "\n",
        "```python\n",
        "@mcp.tool()\n",
        "async def fake_weather(city: str) -> str:\n",
        "    ...\n",
        "```\n",
        "\n",
        "The MCP runtime would then expose a **tool description** (name, description, JSON schema for parameters) to clients.\n",
        "\n",
        "Here, we simulate that with a simple Python dict that represents what `list_tools()` might return.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Simulated MCP \"server\" tool definitions (what list_tools() might give)\n",
        "\n",
        "mcp_tools = [\n",
        "    {\n",
        "        \"name\": \"fake_weather\",\n",
        "        \"description\": \"Return a rough current temperature for a city.\",\n",
        "        \"inputSchema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"city\": {\"type\": \"string\", \"description\": \"City name\"}\n",
        "            },\n",
        "            \"required\": [\"city\"],\n",
        "        },\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"echo\",\n",
        "        \"description\": \"Echo back a message.\",\n",
        "        \"inputSchema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"message\": {\"type\": \"string\", \"description\": \"Message to echo\"}\n",
        "            },\n",
        "            \"required\": [\"message\"],\n",
        "        },\n",
        "    },\n",
        "]\n",
        "\n",
        "print(\"Simulated MCP tools (list_tools() output):\")\n",
        "for t in mcp_tools:\n",
        "    print(f\"- {t['name']}: {t['description']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. MCP Client: Converting Tools \u2192 OpenAI Tool Schema\n",
        "\n",
        "An MCP client does not send MCP protocol details to the model. Instead it converts tools into\n",
        "the **OpenAI `tools` format** (or Anthropic `tools`, etc.).\n",
        "\n",
        "This is what `OpenAIQueryHandler._get_tools_for_openai()` does conceptually:\n",
        "\n",
        "- For each MCP tool:\n",
        "  - Use `name`, `description`, and `inputSchema`.\n",
        "  - Build a `{\"type\": \"function\", \"function\": {...}}` entry for OpenAI.\n",
        "\n",
        "Let's simulate that transformation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def mcp_tools_to_openai_tools(mcp_tools):\n",
        "    \"\"\"Convert simulated MCP tools into OpenAI-style tools list.\"\"\"\n",
        "    openai_tools = []\n",
        "    for tool in mcp_tools:\n",
        "        openai_tools.append(\n",
        "            {\n",
        "                \"type\": \"function\",\n",
        "                \"function\": {\n",
        "                    \"name\": tool[\"name\"],\n",
        "                    \"description\": tool[\"description\"],\n",
        "                    \"parameters\": tool[\"inputSchema\"],\n",
        "                },\n",
        "            }\n",
        "        )\n",
        "    return openai_tools\n",
        "\n",
        "openai_tools = mcp_tools_to_openai_tools(mcp_tools)\n",
        "\n",
        "import json\n",
        "print(\"OpenAI tools schema that would be sent to the model:\\n\")\n",
        "print(json.dumps(openai_tools, indent=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. First Model Call: Choosing a Tool (Tool Call Decision)\n",
        "\n",
        "Now imagine we send this to an OpenAI model:\n",
        "\n",
        "- `messages = [{\"role\": \"user\", \"content\": \"What is the weather in London?\"}]`\n",
        "- `tools = openai_tools` (from above)\n",
        "\n",
        "The model will read the tools and think:\n",
        "\n",
        "> *\"User asked about weather. I see a tool called `fake_weather(city)`. I should call it with `city=\"London\"`.\"*\n",
        "\n",
        "The model then **does not send back the schema**.\n",
        "Instead, it sends back **only the tool name + arguments** in a `tool_calls` structure.\n",
        "\n",
        "We simulate this behavior with a simple function instead of actually calling OpenAI.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def fake_model_decide_tool(user_query: str, tools: list[dict]) -> dict:\n",
        "    \"\"\"Simulate a tool-calling model's first response.\n",
        "\n",
        "    It looks at the text and decides which tool to call and with what arguments.\n",
        "    For teaching, we implement a simple rule-based version.\n",
        "    \"\"\"\n",
        "    user_query_lower = user_query.lower()\n",
        "\n",
        "    if \"weather\" in user_query_lower:\n",
        "        # Pretend the model parsed the city name \"London\" from the question.\n",
        "        return {\n",
        "            \"role\": \"assistant\",\n",
        "            \"tool_calls\": [\n",
        "                {\n",
        "                    \"id\": \"call_1\",\n",
        "                    \"type\": \"function\",\n",
        "                    \"function\": {\n",
        "                        \"name\": \"fake_weather\",\n",
        "                        \"arguments\": json.dumps({\"city\": \"London\"}),\n",
        "                    },\n",
        "                }\n",
        "            ],\n",
        "            \"content\": \"Let me check the weather using my tools.\",\n",
        "        }\n",
        "    else:\n",
        "        # No tools used, just answer directly.\n",
        "        return {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": \"I don't think I need tools for this question.\",\n",
        "            \"tool_calls\": [],\n",
        "        }\n",
        "\n",
        "user_query = \"What is the weather in London right now?\"\n",
        "first_model_message = fake_model_decide_tool(user_query, openai_tools)\n",
        "\n",
        "print(\"First model response (note: tool name + arguments, no schema):\\n\")\n",
        "print(json.dumps(first_model_message, indent=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ud83d\udc49 **Key observation:**\n",
        "\n",
        "- The model output contains:\n",
        "  - `tool_calls[0].function.name` \u2192 `\"fake_weather\"`\n",
        "  - `tool_calls[0].function.arguments` \u2192 `{ \"city\": \"London\" }`\n",
        "- It does **not** return the JSON schema.\n",
        "- It does **not** return MCP protocol frames.\n",
        "\n",
        "Schemas only flowed **from client \u2192 model** (as part of the `tools` list).\n",
        "The model's job is just to **pick a tool and fill arguments**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. MCP Client: Executing the Requested Tool\n",
        "\n",
        "Now the **client** inspects `tool_calls` and says:\n",
        "\n",
        "> *\"The model wants to call `fake_weather` with `city=\"London\"`. I will execute that tool.\"*\n",
        "\n",
        "In a real MCP setup:\n",
        "\n",
        "- The client would translate this into an MCP `call_tool` request over stdio or sockets.\n",
        "- The MCP server would run the Python function and return the result.\n",
        "\n",
        "Here, we simulate that by:\n",
        "\n",
        "1. Defining local Python functions `fake_weather` and `echo`.\n",
        "2. Writing a simple `execute_tool_call(...)` that:\n",
        "   - looks up the function by name,\n",
        "   - parses arguments,\n",
        "   - executes the function,\n",
        "   - returns a `role=\"tool\"` message (like OpenAI expects).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from typing import Callable\n",
        "\n",
        "# 1. Define the actual underlying tool implementations (like MCP server functions)\n",
        "\n",
        "def tool_fake_weather(city: str) -> str:\n",
        "    temps = {\"London\": 18, \"Hyderabad\": 32, \"Bangalore\": 26}\n",
        "    temp = temps.get(city, 25)\n",
        "    return f\"It is roughly {temp}\u00b0C in {city} right now.\"\n",
        "\n",
        "\n",
        "def tool_echo(message: str) -> str:\n",
        "    return f\"Echo: {message}\"\n",
        "\n",
        "\n",
        "# 2. Registry mapping tool names \u2192 functions\n",
        "TOOL_REGISTRY: dict[str, Callable[..., str]] = {\n",
        "    \"fake_weather\": tool_fake_weather,\n",
        "    \"echo\": tool_echo,\n",
        "}\n",
        "\n",
        "\n",
        "def execute_tool_call(tool_call: dict) -> dict:\n",
        "    \"\"\"Simulate MCP client executing an MCP tool and returning a tool message.\n",
        "\n",
        "    This is where, in reality, MCP protocol would be used.\n",
        "    Here we just call local Python functions for clarity.\n",
        "    \"\"\"\n",
        "    tool_name = tool_call[\"function\"][\"name\"]\n",
        "    args = json.loads(tool_call[\"function\"][\"arguments\"] or \"{}\")\n",
        "    tool_fn = TOOL_REGISTRY.get(tool_name)\n",
        "    if tool_fn is None:\n",
        "        content = f\"Error: unknown tool '{tool_name}'\"\n",
        "    else:\n",
        "        try:\n",
        "            content = tool_fn(**args)\n",
        "        except Exception as e:  # noqa: BLE001\n",
        "            content = f\"Error executing tool {tool_name}: {e}\"\n",
        "\n",
        "    # This mimics an OpenAI \"tool\" message that would be appended to the chat.\n",
        "    tool_message = {\n",
        "        \"role\": \"tool\",\n",
        "        \"tool_call_id\": tool_call[\"id\"],\n",
        "        \"content\": content,\n",
        "    }\n",
        "    return tool_message\n",
        "\n",
        "\n",
        "# Execute all tool calls from the first model message\n",
        "tool_messages = []\n",
        "for tc in first_model_message.get(\"tool_calls\", []):\n",
        "    tm = execute_tool_call(tc)\n",
        "    tool_messages.append(tm)\n",
        "\n",
        "print(\"Tool messages produced by client (after executing tools):\\n\")\n",
        "print(json.dumps(tool_messages, indent=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Second Model Call: Using Tool Results to Answer\n",
        "\n",
        "Now the client has:\n",
        "\n",
        "- Original user message.\n",
        "- Assistant's tool call message (\"Let me check the weather using my tools.\").\n",
        "- One or more `role=\"tool\"` messages with the actual tool outputs.\n",
        "\n",
        "In a real system, the client would send all this back to the LLM and ask:\n",
        "\n",
        "> *\"Here are the tool results. Now give a final answer to the user.\"*\n",
        "\n",
        "We'll simulate that with another fake model function `fake_model_final_answer(...)`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def fake_model_final_answer(user_query: str, tool_messages: list[dict]) -> dict:\n",
        "    \"\"\"Simulate the second model call that produces the final answer.\n",
        "\n",
        "    Here we keep it very simple: just weave the tool content into a natural sentence.\n",
        "    \"\"\"\n",
        "    if not tool_messages:\n",
        "        return {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": \"I did not use any tools. (Demo)\",\n",
        "        }\n",
        "\n",
        "    tool_content = tool_messages[0][\"content\"]\n",
        "    answer = (\n",
        "        f\"You asked: '{user_query}'.\\n\"\n",
        "        f\"Based on the tool result, here is the answer: {tool_content}\"\n",
        "    )\n",
        "    return {\"role\": \"assistant\", \"content\": answer}\n",
        "\n",
        "\n",
        "final_message = fake_model_final_answer(user_query, tool_messages)\n",
        "\n",
        "print(\"Final assistant message (second model call simulation):\\n\")\n",
        "print(json.dumps(final_message, indent=2))\n",
        "print(\"\\nAs plain text:\\n\")\n",
        "print(final_message[\"content\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Summary: Where Is MCP, Where Is the Model?\n",
        "\n",
        "In this notebook, we simulated the entire flow **without any real network calls**, but the pattern is the same in real MCP+OpenAI setups:\n",
        "\n",
        "1. **MCP server** defines tools and exposes `list_tools()` and `call_tool()`.\n",
        "2. **MCP client**:\n",
        "   - Calls `list_tools()` \u2192 gets tool names + JSON schemas.\n",
        "   - Converts them to OpenAI `tools` format.\n",
        "3. **First model call**:\n",
        "   - Input: user message + tools.\n",
        "   - Output: (optionally) `tool_calls` with tool name + JSON arguments.\n",
        "   - \u274c No schemas returned by the model.\n",
        "4. **Client executes tools**:\n",
        "   - Uses MCP `call_tool` protocol in reality (here: local Python calls).\n",
        "   - Builds `role=\"tool\"` messages with the tool outputs.\n",
        "5. **Second model call**:\n",
        "   - Input: conversation so far + tool messages.\n",
        "   - Output: final natural-language answer.\n",
        "\n",
        "\ud83d\udc49 The illusion that **\"LLM talks MCP\"** comes from the client hiding all protocol\n",
        "details and giving the model just a **menu of tools** and the **results** of those tools.\n",
        "\n",
        "The model:\n",
        "\n",
        "- Never crafts MCP JSON-RPC frames.\n",
        "- Never sends back JSON schemas.\n",
        "- Only chooses **which tool** to call and **what arguments** to pass.\n",
        "\n",
        "You can now use this notebook to walk students cell by cell through the entire story.\n",
        "Encourage them to modify the tools, change the query text, or add new tools to see how\n",
        "the flow generalizes.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}